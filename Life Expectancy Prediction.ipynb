{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install pycountry_convert\n",
    "import pycountry_convert as pc\n",
    "!pip install plotly\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Opening the file and cleaning and cleaning the data frame.\n",
    "life_exp = pd.read_csv(os.path.join(os.getcwd(), \"Life Expectancy Data.csv\"),encoding=\"utf8\")\n",
    "life_exp.columns = life_exp.columns.str.strip()\n",
    "life_exp.rename(columns={'Life expectancy' : 'Life Expectancy'}, inplace=True)\n",
    "#Pycountry (which will be used to obtain the continents of each country) has slightly different names for each country.\n",
    "#As a result, they have been renamed in the for loop below. \n",
    "life_exp[\"Continent\"] = \"\"\n",
    "for index in range(0, len(life_exp)):\n",
    "    if life_exp.loc[index, \"Country\"] == 'Bolivia (Plurinational State of)':\n",
    "        life_exp.loc[index, \"Country\"] = 'Bolivia, Plurinational State of'\n",
    "    elif life_exp.loc[index, \"Country\"] == 'Iran (Islamic Republic of)':\n",
    "        life_exp.loc[index, \"Country\"] = 'Iran, Islamic Republic of'\n",
    "    elif life_exp.loc[index, \"Country\"] == 'Micronesia (Federated States of)':\n",
    "        life_exp.loc[index, \"Country\"] = 'Micronesia, Federated States of'\n",
    "    elif life_exp.loc[index, \"Country\"] == 'Republic of Korea':\n",
    "        life_exp.loc[index, \"Country\"] = 'Korea, Republic of'\n",
    "    elif life_exp.loc[index, \"Country\"] == 'The former Yugoslav republic of Macedonia':\n",
    "        life_exp.loc[index, \"Country\"] = 'North Macedonia'\n",
    "    elif life_exp.loc[index, \"Country\"] == \"Timor-Leste\":\n",
    "        continent_name = pc.country_alpha2_to_continent_code(\"TP\")\n",
    "        life_exp.loc[index, \"Continent\"] = continent_name\n",
    "        continue\n",
    "    elif life_exp.loc[index, \"Country\"] == 'Venezuela (Bolivarian Republic of)':\n",
    "        life_exp.loc[index, \"Country\"] = 'Venezuela, Bolivarian Republic of'\n",
    "    #The following lines of code are from https://stackoverflow.com/questions/55910004/get-continent-name-from-country-using-pycountry\n",
    "    #The code will will mark the continent each country resides in.\n",
    "    country_code = pc.country_name_to_country_alpha2(life_exp.loc[index,\"Country\"], cn_name_format = \"default\")\n",
    "    continent_name = pc.country_alpha2_to_continent_code(country_code)\n",
    "    life_exp.loc[index, \"Continent\"] = continent_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before going any further, let's look at our data frame.\n",
    "## Let's also take a look at an explanation of each column, based on the descriptions from the data's [source](https://www.kaggle.com/kumarajarshi/life-expectancy-who?ref=hackernoon.com)\n",
    "    Status-\"Developed or Developing status\"\n",
    "    Life Expectancy-\"Life Expectancy in age\"\n",
    "    Adult Mortality-\"Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)\"\n",
    "    Infant Deaths-\"Number of Infant Deaths per 1000 population\"\n",
    "    Alcohol-\"Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)\"\n",
    "    Percentage Expenditure-\"Expenditure on health as a percentage of Gross Domestic Product per capita(%)\"\n",
    "    Hepatitis B-\"Hepatitis B (HepB) immunization coverage among 1-year-olds (%)\"\n",
    "    Measles-\"Measles - number of reported cases per 1000 population\"\n",
    "    BMI-\"Average Body Mass Index of entire population\"\n",
    "    Under Five Deaths-\"Number of under-five deaths per 1000 population\"\n",
    "    Polio-\"Polio (Pol3) immunization coverage among 1-year-olds (%)\"\n",
    "    Total Expenditure-\"General government expenditure on health as a percentage of total government expenditure (%)\"\n",
    "    Diphtheria-\"Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\"\n",
    "    HIV/AIDS-\"Deaths per 1 000 live births HIV/AIDS (0-4 years)\"\n",
    "    GDP-\"Gross Domestic Product per capita (in USD)\"\n",
    "    Population-\"Population of the country\"\n",
    "    Thinness 1-19 Years-\"Prevalence of thinness among children and adolescents for Age 10 to 19 (%)\"\n",
    "    Thinness 5-9 Years-\"Prevalence of thinness among children for Age 5 to 9(%)\"\n",
    "    Income Composition of Resources-\"Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\"\n",
    "    Schooling-\"Number of years of Schooling(years)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "life_exp[\"Country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A temporary data frame, v will be created to obtain the yearly average life expectancy for all the continents combined. \n",
    "all_continents = life_exp.groupby(\"Year\").mean().reset_index()\n",
    "all_continents[\"Continent\"] = \"All Continents\"\n",
    "#The for loop below will find the yearly average life expectancy for each continent and then merge it with the all_continents data frame.\n",
    "for continent in list(life_exp[\"Continent\"].unique()):\n",
    "    continent_df = life_exp[life_exp[\"Continent\"] == continent].groupby(\"Year\").mean().reset_index()\n",
    "    continent_df[\"Continent\"] = str(continent)\n",
    "    all_continents = pd.merge(all_continents,continent_df,how=\"outer\")\n",
    "#At this point, the all_continents data frame has the annual average life expectancy for each continent as well as the annual average life expectancy for all the continents combined. \n",
    "life_exp_fig = px.line(all_continents, x=\"Year\", y=\"Life Expectancy\", color=\"Continent\",title='Life Expectency Across Continents')\n",
    "life_exp_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The function below will take a metric like GDP and compare it across continents. \n",
    "def bar_graph(metric): \n",
    "    temp_dict={}\n",
    "    #The for loop below creates a dictionary where the keys are the continients and the values are the mean metric of each continent.\n",
    "    #If the metric passed is GDP, then the values are the mean GDP for each continent.s\n",
    "    for cont in life_exp[\"Continent\"].unique():\n",
    "        temp_dict[cont]=life_exp[life_exp[\"Continent\"]==cont][metric].mean()\n",
    "    #Create a dataframe from the dictionary and formatting it.\n",
    "    DF=pd.DataFrame(temp_dict,index=[metric])\n",
    "    DF=DF.transpose()\n",
    "    DF=DF.reset_index()\n",
    "    DF=DF.rename(columns={\"index\": \"Continent\"})\n",
    "    DF=DF.sort_values(by=[metric])\n",
    "    fig = px.bar(DF, x=\"Continent\", y=metric,color=metric,title=\"Average \"+metric+' Across Continents')\n",
    "    fig.show()\n",
    "bar_graph(\"Life Expectancy\")\n",
    "bar_graph(\"GDP\")\n",
    "bar_graph(\"Adult Mortality\")\n",
    "bar_graph(\"Income composition of resources\")\n",
    "bar_graph(\"Schooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.pie(life_exp,values=\"Population\",names=\"Continent\",title=\"Population Share of Each Continent\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before going any further, it will be useful to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code for filling in nan values is based off of the following video: https://www.youtube.com/watch?v=J_LnPL3Qg70sy\n",
    "independent = life_exp.drop(['Life Expectancy',\"Country\"],axis = 1) #Removing the dependent variables\n",
    "values = {}\n",
    "for col in independent: #Filling the NaN values\n",
    "    if independent[col].isnull().values.any():\n",
    "        values[col] = independent[col].mean()\n",
    "independent.fillna(value = values, inplace=True)\n",
    "\n",
    "life_exp[\"Life Expectancy\"].fillna(value=life_exp[\"Life Expectancy\"].mean(), inplace=True)\n",
    "\n",
    "#The following for loops will convert the columns \"Continent\" and \"Status\" to numerical values using one hot encoding.\n",
    "for cont in independent[\"Continent\"].unique():\n",
    "    new_col=\"Continent_\"+cont\n",
    "    independent[new_col]=0\n",
    "    independent.loc[independent['Continent'] == cont, new_col]=1 #This filters the data frame by each continent, then sets the the continent's column to 1.\n",
    "    #E.g. The data frame is filtered so that only countries in Africa are present. Then, it sets the column \"Continent_AF\" to 1. \n",
    "    \n",
    "for status in independent[\"Status\"].unique():\n",
    "    new_status=\"Status_\"+status\n",
    "    independent[new_status]=0\n",
    "    independent.loc[independent['Status'] == status, new_status]=1 \n",
    "#Now that we've encoded the \"Continent\" and \"Status\" variables, we can drop them. \n",
    "#We want to drop them because we can't use non-numerical values in linear regression.\n",
    "independent=independent.drop(\"Continent\",axis=1)\n",
    "independent=independent.drop(\"Status\",axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Assumptions of Multiple Linear Regression Using Ordinary Least Squares (OLS) Regression\n",
    "## To ensure that our model is the Best Linear Unbiased Estimator, we need to handle some key assumptions. If an assumption is violated, we need to handle them as best as we can to increase confidence in our predictions. \n",
    "## The following are the assumptions we will be dealing with:<br>1. Linearity <br>2. No Multicollinearity <br>3. Homoscedasticity <br>4. No Autocorrelation <br>5. Mean Residual of Zero "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Multicollinearity:\n",
    "### There may be variables that are strongly correlated with each other. If two variables are highly correlated, we should remove one to avoid the issues that accompany multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df): \n",
    "    # The code used tocreate this correlation matrix is from https://towardsdatascience.com/introduction-to-data-visualization-in-python-89a54c97fbed\n",
    "    # Creates a correlation matrix\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    # create heatmap\n",
    "    im = ax.imshow(corr.values)\n",
    "    plt.colorbar(im)\n",
    "    # set labels\n",
    "    ax.set_xticks(np.arange(len(corr.columns)))\n",
    "    ax.set_yticks(np.arange(len(corr.columns)))\n",
    "    ax.set_xticklabels(corr.columns, fontsize=15)\n",
    "    ax.set_yticklabels(corr.columns,fontsize=15)\n",
    "    ax.set_title(\"Correlation Matrix\", fontsize=20)\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    plt.show()\n",
    "correlation_matrix(independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematically Rooting Out Multicollinearity\n",
    "### We can use the variance inflation factor with the Statsmoels library to figure out which variables have high multicollinearity. The following is an excerpt from the its [documentation](statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html) on the variance inflation factor: \n",
    "    The variance inflation factor is a measure for the increase of the variance of the parameter estimates if an additional variable, given by exog_idx is added to the linear regression. It is a measure for multicollinearity of the design matrix, exog.\n",
    "    One recommendation is that if VIF is greater than 5, then the explanatory variable given by exog_idx is highly collinear with the other explanatory variables, and the parameter estimates will have large standard errors because of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code used to implement the variance inflation factor is from https://github.com/areed1192/sigma_coding_youtube/blob/master/python/python-data-science/machine-learning/multi-linear-regression/Machine%20Learning%20-%20Multi%20Linear%20Regression%20Analysis.ipynb\n",
    "X1 = sm.tools.add_constant(independent)\n",
    "X1=X1.astype('float64')\n",
    "series_before = pd.Series([variance_inflation_factor(X1.values, i) for i in range(X1.shape[1])], index=X1.columns)\n",
    "\"\"\"\n",
    "# The line above runs variance_inflation_factor through each cell of each row of X1. \n",
    "# X1.values is a matrix of all the cells (explanatory variables) in each row. \n",
    "# i simply tells the program which row we are at. \n",
    "\"\"\"\n",
    "display(series_before) # This will display the variance inflation factor for each variable. Variables with a factor above 5 should be dropped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, several variables have a variance influence factor above 5. However, we shouldn't eliminate them all at once. Instead, we will eliminate them one at a time until every variable has a factor below 5, preserving as many variables as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_independent = independent\n",
    "series_after = series_before\n",
    "\n",
    "while series_after.sort_values(ascending=False)[1]>5: #While there are variables with a variance influence factor above 5, the while loop continues.\n",
    "    names = list(series_after.sort_values(ascending=False).index) #Sorting the series by descending values guarantees that the first item in the list will have a variance influence factor above 5\n",
    "    new_independent=new_independent.drop(names[1], axis=1) #Drops the variable with an influence factor above 5.\n",
    "    #The following two lines of code are from https://github.com/areed1192/sigma_coding_youtube/blob/master/python/python-data-science/machine-learning/multi-linear-regression/Machine%20Learning%20-%20Multi%20Linear%20Regression%20Analysis.ipynb. \n",
    "    X2 = sm.tools.add_constant(new_independent) #Creates a new constant as we just removed a variable.\n",
    "    X2 = X2.astype(\"float64\")\n",
    "    series_after = pd.Series([variance_inflation_factor(X2.values, i) for i in range(X2.shape[1])], index=X2.columns)\n",
    "display(series_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at our new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The new correlation matrix has far fewer green-yellow squares (green-yellow squares indicate a high correlation between variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(new_independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Heteroscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first method we will use is the Goldfeld–Quandt test. While there is a function that can perform the test, we will review the steps necessary to perform the test to understand what is going on. The steps are based on the function's [source code](https://www.statsmodels.org/stable/_modules/statsmodels/stats/diagnostic.html#het_goldfeldquandt).\n",
    "    Divide the dataset into two sections. For each section, do the following: \n",
    "1. Fit a regression model to the data.\n",
    "2. Use the regression model to predict the y values.\n",
    "3. Find the mean squared error. To compute this, find the difference between the predicted value of y and the actual value of y, then square it. Do this for all the predicted/actual values of y, and then add them all up. This will be the sum of squared errors. Divide the sum by the sample size to find the mean squared error (MSE).\n",
    "4. Compare the MSE for both sections by dividing them. This will become the ratio of mean squared residual errors.\n",
    "5. Compute the residual degree of freedom (DF). This is calculated with the following formula: DF=N-K-1 where N is the number of samples and K is the number of parameters.\n",
    "6. Use the ratio of mean squared residual errors along with the residual degree of freedom to calculate our p-value. If p<.05, then we can reject our null hypothesis (that the variance is about the same throughout). In other words, if p<.05, then heteroscedasticity is likely to present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GQ_test = sm.stats.diagnostic.het_goldfeldquandt(life_exp[\"Life Expectancy\"].astype('float64'), new_independent.astype('float64')) \n",
    "if GQ_test[1]>.05:\n",
    "    print (\"Heteroscedasticity is likely not present. p > .05\")\n",
    "else:\n",
    "    print (\"Heteroscedasticity is likely present. p <= .05\")\n",
    "print (GQ_test[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another method to check for heteroscedasticity is the Breusch-Pagan-Godfrey test. <br><br> The steps are based on the function's [source code](https://www.statsmodels.org/stable/_modules/statsmodels/stats/diagnostic.html#het_breuschpagan).\n",
    "1. Create a linear regression model with the dataset and fit a line to it.\n",
    "2. Capture the residuals of this model into a list or array. \n",
    "3. Create a second linear regression model. This time, Y will not be life expectancy, but rather the residuals from the original model. In other words, we will be using the X variable to try and predict the residuals. Again, we will fit a line to this model. \n",
    "4. Using the second model, we can find the p-value. \n",
    "\n",
    "    Note: The function used below for the test will return two p-values. One will be the p-value derived from the f-statistic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run this test, we need to run a regression and obtain its residuals. The code below takes a random sample from the \n",
    "#data frame and runs a regression on it. We will run a test regression a couple of times, so we will create a function that does this.\n",
    "def test_model():\n",
    "    X_train, X_test, y_train, y_test=train_test_split(new_independent, life_exp[\"Life Expectancy\"], test_size=int(len(new_independent)*.25)) # Spliting the data into training and testing datasets.\n",
    "    test_model = sm.OLS(y_train, X_train.astype(\"float\")) \n",
    "    return test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_test = sm.stats.diagnostic.het_breuschpagan(test_model().fit().resid, test_model().exog, robust=True)\n",
    "if BP_test[1]>.05 and BP_test[3]>.05:\n",
    "    print (\"Heteroscedasticity is likely not present. p > .05\")\n",
    "else:\n",
    "    print (\"Heteroscedasticity is likely present. p <= .05\")\n",
    "print (\"p =\", BP_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It seems as though heteroscedasticity might be present. \n",
    "### This means that the range of y values is not consistent. Some areas will have a larger range of y values and some will have a small range. This is an issue because we will have less information in certain areas. If there is more variance in the dependent variable as the independent variable(s) increase, then we will have less information as it becomes harder to accurately predict the dependent variable. <br><br>While heteroscedasticity does not make OLS regression biased, it does make it [less efficient](https://www.sfu.ca/~pendakur/teaching/buec333/Heteroskedasticity%20and%20Correlations%20Across%20Errors.pdf). Furthermore, heteroscedasticity means that the standard error [will be less accurate](http://www.homepages.ucl.ac.uk/~uctpsc0/Teaching/GR03/Heter&Autocorr.pdf). The standard error is based on the standard deviation as the standard error is standard deviation divided by the square root of the number of samples. Standard deviation is based on variance as it is the square root of the variance in observations. If the variance is inconsistent, then the standard error, which is derived from the variance, [may be inaccurate](https://www.ucl.ac.uk/~uctp41a/b203/lecture9.pdf). Because researchers [often use standard error in hypothesis testing](https://www.biochemia-medica.com/en/journal/18/1/10.11613/BM.2008.002#:~:text=The%20formula%2C%20(1%2DP,correlation%20measure%2C%20the%20Pearson%20R.), inaccurate standard errors will make the conclusions reached less reliable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Autocorrelation\n",
    "### If [autocorrelation](https://corporatefinanceinstitute.com/resources/knowledge/other/autocorrelation/) is present, then one can use one point's residual to predict the next. If there is a positive correlation, then the next point's residual will go in the same direction as the current point's. If the current point has a positive residual, then the next point will likely also have a positive residual. If the current point has a negative residual, then the next point will likely follow suit. With a negative correlation, the next point's residual will go in the opposite direction. If the current point has a positive residual, then it is likely that the next point will have a negative residual and vice versa. <br><br> [Autocorrelation](http://www.homepages.ucl.ac.uk/~uctpsc0/Teaching/GR03/Heter&Autocorr.pdf) does not make OLS regression biased. However, when it is present the variance becomes inconsistent. As aforementioned, inconsistent variance means that the standard errors become less accurate, making hypothesis testing more difficult. Furthermore, it makes the regression [less efficient.](https://www.sfu.ca/~pendakur/teaching/buec333/Heteroskedasticity%20and%20Correlations%20Across%20Errors.pdf)<br><br>We will be using the Durbin Watson statistic to test for autocorrelation. A value between 1.5 and 2.5 indicates no autocorrelation, a value above 2.5 indicates a negative autocorrelation, and a value below 1.5 indicates a positive autocorrelation.<br><br>The test divides the sum of differences squared by the sum of errors squared and [works as follows](https://www.statsmodels.org/stable/_modules/statsmodels/stats/stattools.html#durbin_watson):\n",
    "1. Create and fit a linear regression model.\n",
    "2. Capture the residuals of the model.\n",
    "3. Pass the residuals to the Stats Models function durbin_watson. The following steps will outline how this function works. \n",
    "4. For each item in the array of residuals, calculate the difference between the current and next item.\n",
    "5. Add up all the differences from step 4 and square the sum.\n",
    "6. Add up all the residuals that were passed to the function in step 3 and square the sum.\n",
    "7. Divide the result of step 5 by the result of step 6. Return the dividend. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Like the Breusch-Pagan-Godfrey test, the Durbin Watson test requires that a regression is run and that we obtain the residuals. \n",
    "#We will re-run the regression with a different random sample to get new residuals.\n",
    "\n",
    "if durbin_watson(test_model().fit().resid)>=1.5 and durbin_watson(test_model().fit().resid)<= 2.5:\n",
    "    print (\"Autocorrelation is not an issue.\")\n",
    "elif durbin_watson(test_model().fit().resid)>2.5:\n",
    "    print (\"Autocorrelation is present. There is a negative serial correlation\")\n",
    "elif durbin_watson(test_model().fit().resid)<1.5:\n",
    "    print (\"Autocorrelation is present. There is a positive serial correlation\")\n",
    "print (\"The result of the test is:\",durbin_watson(list(test_model().fit().resid)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Heteroscedasticity\n",
    "## It seems as though heteroscedasticity but not autocorrelation is present in the dataset. We will discuss two methods of handling this issue.<br><br>One way to handle this issue is to run the regression and fit the model using heteroscedasticity robust standard errors. This will [\"obtain unbiased standard errors of OLS coefficients under heteroscedasticity.\"](https://economictheoryblog.com/2016/08/07/robust-standard-errors/)<br>Another option is to use [General Least Squares](http://www.homepages.ucl.ac.uk/~uctpsc0/Teaching/GR03/Heter&Autocorr.pdf) (GLS) regression instead, which has the potential to be more efficient than OLS regression.<br>For this project, we will be going with the former option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Mean Residual Error\n",
    "### We will be checking if the the error the model makes, on average, is close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model().fit().resid.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we can see, the mean error is close to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model and Making Predictions\n",
    "## The formula below displays the coefficients for each variable. For instance, if the HIV/AIDS variable increases by 1, the life expectancy changes by the HIV/AIDS's coefficient. If the coefficient is -.048, then the life expectancy decreases by .048 for every unit of increase in HIV/AIDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will obtain a random sample, add a constant, and run a regression.\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_independent, life_exp[\"Life Expectancy\"], test_size=int(len(new_independent)*.25)) # Spliting the data into training and testing datasets.\n",
    "X_train=sm.add_constant(X_train)\n",
    "X_test=sm.add_constant(X_test)\n",
    "model = sm.OLS(y_train, X_train.astype(\"float\"))\n",
    "results = model.fit(cov_type=\"HAC\",cov_kwds={\"maxlags\":25}) # Fitting the model using heteroscedasticity and autocorrelation robust standard errors\n",
    "intercept = results.params[0]\n",
    "print (\"The linear regression formula is as follows: Life Expectancy = \\n\", intercept)\n",
    "for cf in zip(new_independent.columns, results.params[1:]): #This line is from https://github.com/areed1192/sigma_coding_youtube/blob/master/python/python-data-science/machine-learning/multi-linear-regression/Machine%20Learning%20-%20Multi%20Linear%20Regression%20Analysis.ipynb.\n",
    "    print (\"+\",cf[0], cf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = results.predict(X_test) #This is an array of the predicted values for the testing datset.\n",
    "sc = plt.scatter(y_pred, y_test, c=y_test - y_pred) #The scatterplot is colorcoded based on how different the actual and predicted life expectancies are. \n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Actual Value\")\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Difference')\n",
    "plt.title(\"Predicted vs Actual Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "3. Look over all the notes, and make sure they all make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User input\n",
    "### In this section, you will be able to input your parameters for the independent variables, and the program will predict the life expectancy based on your inputs.<br>Please note, when the program asks a question like \"Please enter the Continent_EU input\", it is asking whether or not the country in question is on the European continent. If it is, please enter 1, otherwise enter 0. Likewise, if the program asks a question like \"Please enter the Status_Developed input\", it is asking whether or not the country in question is developed. If it is, enter 1, otherwise, enter 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user={}\n",
    "valid=False\n",
    "\"\"\"\n",
    "# We will run the regression again, this time on the entire data set, not just a sample. \n",
    "# Previously, we split the data set into training and testing subsets. \n",
    "# We wanted to teach the program with the training subset and gauge its accuracy by comparing the predicted values and actual values of the testing subset.\n",
    "# This time, the entire data set will be the training subset and the user input will be the test.\n",
    "# Since we only have the predicted life expectancy based on the user input, we cannot compare it to the actual life expectancy.\n",
    "\"\"\"\n",
    "results = sm.OLS(life_exp[\"Life Expectancy\"], new_independent.astype(\"float\")).fit(cov_type=\"HAC\",cov_kwds={\"maxlags\":25}) \n",
    "for col in list(new_independent.columns):\n",
    "    valid=False\n",
    "    while valid==False:\n",
    "        info=input(\"Please enter the \"+col+\" input: \")\n",
    "        if \"Continent\" not in col and \"Status\" not in col:\n",
    "            try: \n",
    "                user[col]=float(info)\n",
    "                valid=True\n",
    "            except:\n",
    "                print (\"Hmmm. This seems to be an invalid input.\")\n",
    "        elif \"Continent\" in col or \"Status\" in col:\n",
    "            try:\n",
    "                if int(info)==0 or int(info)==1:\n",
    "                    user[col]=float(info)\n",
    "                    valid=True\n",
    "                else: \n",
    "                    print (\"Hmmm. This seems to be an invalid input.\")\n",
    "            except:\n",
    "                print (\"Hmmm. This seem to be an invalid input.\")\n",
    "\n",
    "life=results.params[0]\n",
    "for key in user:\n",
    "    life+=(user[key]*results.params.loc[key])\n",
    "print (\"Your predicted life expectancy is:\", life, \"years.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'mrrehani' # your username\n",
    "api_key = \"7Kp3W7yUs7C9tKPjoMpr\" # your api key - go to profile > settings > regenerate key\n",
    "import chart_studio\n",
    "chart_studio.tools.set_credentials_file(username=username, api_key=api_key)\n",
    "import chart_studio.plotly as py\n",
    "py.plot(pie_fig, filename = 'fig5', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
